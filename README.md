# Forge-VGT: Vision-Guided Transformer with Forging Optimization

A modular, open-source implementation of the VGT-8L model with dynamic training stabilization via the Forge Controller.

## ğŸ“¦ Features

- **Modular Architecture**: Clean separation of model, dataset, training loop, and optimization logic.
- **Forge Training Strategy**: Adaptive loss scaling (`Î±`) with warm-up, compaction, and annealing phases.
- **Stable Residual Blocks**: GRU-based residual blocks with LayerNorm and dropout.
- **Streaming Dataset**: Efficient line-by-line JSONL data loading for large corpora.
- **Mixed-Precision Training**: AMP (Automatic Mixed Precision) support for faster GPU training.
- **Checkpointing**: Automatic resume from latest checkpoint.
- **Pluggable Loss Function**: Core `compute_forge_loss` is decoupled for easy modification or replacement.

## ğŸ› ï¸ Requirements

- Python â‰¥ 3.8
- PyTorch â‰¥ 2.0
- CUDA-compatible GPU (for training)

Install dependencies:
```bash
pip install torch
```

## ğŸ“ Project Structure

```
Forge-VGT/
â”œâ”€â”€ models/               # Model definitions
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ vgt_8l.py
â”œâ”€â”€ data/                 # Dataset utilities
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ stream_dataset.py
â”œâ”€â”€ training/             # Training logic
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ forge_controller.py
â”‚   â”œâ”€â”€ loss_function.py  # â† Pluggable loss core
â”‚   â””â”€â”€ trainer.py
â”œâ”€â”€ config/               # Configuration files
â”‚   â””â”€â”€ train_config.json
â”œâ”€â”€ vocab.json            # Vocabulary file (required)
â”œâ”€â”€ train_encyclopedia.json  # Training data (JSONL format)
â”œâ”€â”€ train.py              # Main entry point
â””â”€â”€ README.md
```

## ğŸ”Œ Core Loss Function (Pluggable Design)

The heart of Forge-VGT is its composite loss, now fully modular in [`training/loss_function.py`](training/loss_function.py):

```python
def compute_forge_loss(logits, targets, h_states, embedding_layer, vocab_size, alpha):
    # 1. Standard cross-entropy
    ce_loss = F.cross_entropy(logits.view(-1, vocab_size), targets.view(-1))

    # 2. Hidden state norm regularization
    h_norm = torch.sqrt(torch.mean(h_states ** 2))

    # 3. Cosine alignment with target embeddings
    with torch.no_grad():
        target_emb = embedding_layer(targets)
    cos_sim = F.cosine_similarity(h_states, target_emb, dim=-1).mean()
    cos_loss = 1.0 - cos_sim

    # Combine with dynamic Forge scaling
    total_loss = ce_loss + alpha * 0.15 * h_norm + alpha * 0.40 * cos_loss
    return total_loss, { ... }
```

This design allows researchers to:
- Swap in alternative regularization terms
- Adjust weighting coefficients without touching the trainer
- Reuse the loss in other architectures

## â–¶ï¸ Quick Start

1. Prepare your vocabulary (`vocab.json`) and training data (`train_encyclopedia.json` in JSONL format).
2. Run training:
   ```bash
   python train.py
   ```
3. Checkpoints will be saved as `vgt_8L_step_{step}.pth`.

## ğŸ“œ License

MIT License. See `LICENSE` for details.

## ğŸŒŸ Acknowledgements

Inspired by adaptive regularization and representation forging techniques in modern language modeling.